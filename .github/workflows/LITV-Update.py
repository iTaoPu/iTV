import os
import aiohttp
import asyncio
import time
from collections import defaultdict
import re
from datetime import datetime, timedelta


def get_dynamic_keywords():
    """
    åŠ¨æ€ç”Ÿæˆéœ€è¦è¿‡æ»¤çš„å…³é”®è¯ï¼ˆä»Šå¤©çš„æ—¥æœŸã€æ˜å¤©çš„æ—¥æœŸä»¥åŠå›ºå®šå…³é”®è¯ï¼‰
    """
    # è·å–ä»Šå¤©å’Œæ˜å¤©çš„æ—¥æœŸ
    today = datetime.now().strftime("%Y-%m-%d")
    tomorrow = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
    
    fixed_keywords = ["å…è´¹æä¾›", today, tomorrow]
    return fixed_keywords

def contains_date(text):
    """
    æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ï¼ˆå¦‚ YYYY-MM-DDï¼‰
    """
    date_pattern = r"\d{4}-\d{2}-\d{2}"  # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… YYYY-MM-DD
    return re.search(date_pattern, text) is not None


# é…ç½®
CONFIG = {
    "timeout": 10,  # Timeout in seconds
    "max_parallel": 30,  # Max concurrent requests
    "output_m3u": "LITV.m3u",  # Output file for the sorted M3U
    "output_txt": "LITV.txt",  # Output file for the TXT format
    "iptv_directory": "IPTV",  # Directory containing IPTV files
    "logo_base_url": "https://itv.shrimp.cloudns.biz/logo"  # Base URL for logos
}


# è¯»å– CCTV é¢‘é“åˆ—è¡¨
def load_cctv_channels(file_path=".github/workflows/iTV/CCTV.txt"):
    """ä»æ–‡ä»¶åŠ è½½ CCTV é¢‘é“åˆ—è¡¨"""
    cctv_channels = set()
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if line:  # Ignore empty lines
                    cctv_channels.add(line)
    except FileNotFoundError:
        print(f"Error: The file {file_path} was not found.")
    return cctv_channels


# è¯»å– IPTV ç›®å½•ä¸‹æ‰€æœ‰çœä»½é¢‘é“æ–‡ä»¶
def load_province_channels(files):
    """åŠ è½½å¤šä¸ªçœä»½çš„é¢‘é“åˆ—è¡¨"""
    province_channels = defaultdict(set)

    for file_path in files:
        province_name = os.path.basename(file_path).replace(".txt", "")  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºçœä»½åç§°

        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                for line in file:
                    line = line.strip()
                    if line:  # å¿½ç•¥ç©ºè¡Œ
                        province_channels[province_name].add(line)
        except FileNotFoundError:
            print(f"Error: The file {file_path} was not found.")

    return province_channels


# æ­£è§„åŒ–é¢‘é“åç§°ï¼Œç”ŸæˆLogoæ–‡ä»¶å
def normalize_logo_name(channel_name):
    """å°†é¢‘é“åç§°æ­£è§„åŒ–ï¼Œåªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼Œç”¨äºLogoæ–‡ä»¶å"""
    # é¦–å…ˆè¿›è¡ŒåŸºæœ¬çš„æ­£è§„åŒ–å¤„ç†
    normalized = re.sub(r'[^\w\s]', '', channel_name)  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    normalized = re.sub(r'\s+', '', normalized)  # ç§»é™¤ç©ºæ ¼
    
    # æ›¿æ¢ç‰¹å®šçš„CCTVæ ¼å¼
    normalized = re.sub(r'CCTV[-]?(\d+)(?:ç»¼åˆ|æ–°é—»|è´¢ç»|ç»¼è‰º|ä½“è‚²|ç”µå½±|ç”µè§†å‰§|æˆæ›²|éŸ³ä¹|ç§‘æ•™|å°‘å„¿)?', r'CCTV\1', normalized)
    
    return normalized


# æ­£è§„åŒ– CCTV é¢‘é“åç§°
def normalize_cctv_name(channel_name):
    """å°† CCTV é¢‘é“åç§°è¿›è¡Œæ­£è§„åŒ–ï¼Œä¾‹å¦‚ CCTV-1 -> CCTV1"""
    return re.sub(r'CCTV[-]?(\d+)', r'CCTV\1', channel_name)


# ä» TXT æ–‡ä»¶ä¸­æå– IPTV é“¾æ¥
def extract_urls_from_txt(content):
    """ä» TXT æ–‡ä»¶ä¸­æå– IPTV é“¾æ¥"""
    urls = []
    for line in content.splitlines():
        line = line.strip()
        if line and ',' in line:  # æ ¼å¼åº”è¯¥æ˜¯: <é¢‘é“å>,<URL>
            parts = line.split(',', 1)
            urls.append((parts[0], parts[1], None))  # æå–é¢‘é“åã€URLå’Œlogo (TXTæ²¡æœ‰logo)
    return urls


# ä» M3U æ–‡ä»¶ä¸­æå– IPTV é“¾æ¥
def extract_urls_from_m3u(content):
    """ä» M3U æ–‡ä»¶ä¸­æå– IPTV é“¾æ¥åŠåŸå§‹logo"""
    urls = []
    lines = content.splitlines()
    current_channel = "Unknown"
    current_logo = None  # å­˜å‚¨å½“å‰é¢‘é“çš„åŸå§‹logo

    for line in lines:
        line = line.strip()
        if line.startswith("#EXTINF:"):
            # è§£æé¢‘é“ä¿¡æ¯
            current_logo = None  # é‡ç½®logo
            # å°è¯•æå–tvg-logoå±æ€§
            match = re.search(r'tvg-logo="([^"]+)"', line)
            if match:
                current_logo = match.group(1)
                
            # æå–é¢‘é“åç§°ï¼ˆé€—å·åçš„éƒ¨åˆ†ï¼‰
            parts = line.split(',', 1)
            current_channel = parts[1] if len(parts) > 1 else "Unknown"
            
        elif line.startswith(('http://', 'https://')):
            # å­˜å‚¨é¢‘é“åã€URLå’ŒåŸå§‹logoï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            urls.append((current_channel, line, current_logo))
            current_logo = None  # é‡ç½®å½“å‰logo
    return urls


# æµ‹è¯•å¤šä¸ª IPTV é“¾æ¥çš„å¯ç”¨æ€§å’Œé€Ÿåº¦ï¼ˆå¯é€‰ï¼‰
async def test_multiple_streams(urls):
    """æµ‹è¯•å¤šä¸ª IPTV é“¾æ¥ï¼ˆå¯é€‰ï¼‰"""
    return [(True, 0.0)] * len(urls)  # æ€»æ˜¯è¿”å›æ‰€æœ‰é“¾æ¥éƒ½æœ‰æ•ˆ


# è¯»å–æ–‡ä»¶å¹¶æå– URLï¼ˆæ”¯æŒ M3U æˆ– TXT æ ¼å¼ï¼‰
async def read_and_test_file(file_path, is_m3u=False):
    """è¯»å–æ–‡ä»¶å¹¶æå–æ‰€æœ‰ URLï¼ˆä¸è¿‡æ»¤ï¼‰"""
    try:
        # è·å–æ–‡ä»¶å†…å®¹
        async with aiohttp.ClientSession(cookie_jar=None) as session:  # ç¦ç”¨ cookie å¤„ç†
            async with session.get(file_path) as response:
                content = await response.text()

        # æå– URL
        if is_m3u:
            entries = extract_urls_from_m3u(content)
        else:
            entries = extract_urls_from_txt(content)

        # ç›´æ¥è¿”å›æ‰€æœ‰ URLï¼ˆä¸è¿‡æ»¤ï¼‰
        return entries

    except Exception as e:
        return []


# ç”Ÿæˆæ’åºåçš„ M3U æ–‡ä»¶å’Œ TXT æ–‡ä»¶
def generate_output_files(valid_urls, cctv_channels, province_channels, m3u_filename, txt_filename):
    """ç”Ÿæˆæ’åºåçš„ M3U æ–‡ä»¶å’Œ TXT æ–‡ä»¶ï¼ˆTXT æŒ‰ç…§åˆ†ç»„ç»“æ„è¾“å‡ºï¼‰"""
    cctv_channels_list = []
    province_channels_list = defaultdict(list)
    satellite_channels = []
    other_channels = []
    
    # æ„å»ºå››è¿å­—ç´¢å¼•ï¼ˆä¼˜åŒ–åŒ¹é…å‡†ç¡®ç‡ï¼‰
    quadgram_to_province = defaultdict(set)

    # è·å–åŠ¨æ€å…³é”®è¯ï¼Œç”¨äºè¿‡æ»¤å«æ—¶é—´åå­—çš„æº
    filter_keywords = get_dynamic_keywords()

    # éå†æ‰€æœ‰çœä»½çš„æ‰€æœ‰é¢‘é“ï¼Œæ„å»ºå››è¿å­—ç´¢å¼•
    for province, channels in province_channels.items():
        for channel_name in channels:
            # æ·»åŠ åŸå§‹è¯åºçš„å››è¿å­—
            if len(channel_name) >= 4:
                # ä¸ºé¢‘é“ååˆ›å»ºæ‰€æœ‰å¯èƒ½çš„å››è¿å­—ç»„åˆ
                for i in range(len(channel_name) - 3):
                    quadgram = channel_name[i:i+4]
                    quadgram_to_province[quadgram].add(province)

    # å¤„ç†æ‰€æœ‰æœ‰æ•ˆçš„URLï¼Œè¿‡æ»¤å«æ—¶é—´åå­—çš„æº
    for channel, url, orig_logo in valid_urls:
        # è¿‡æ»¤åŒ…å«æ—¥æœŸæˆ–å…³é”®è¯çš„æº
        if contains_date(channel) or any(keyword in channel for keyword in filter_keywords):
            continue  # è·³è¿‡å«æ—¶é—´åå­—çš„æº
        
        # æ­£è§„åŒ–é¢‘é“åç§°ï¼Œä½œä¸ºLogoæ–‡ä»¶å
        logo_name = normalize_logo_name(channel)
        
        # ç”ŸæˆLogo URL
        logo_url = f"{CONFIG['logo_base_url']}/{logo_name}.png"
        
        # æ­£è§„åŒ– CCTV é¢‘é“å
        normalized_channel = normalize_cctv_name(channel)

        # æ ¹æ®é¢‘é“ååˆ¤æ–­å±äºå“ªä¸ªåˆ†ç»„
        found_province = None
        
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯CCTVé¢‘é“
        if normalized_channel in cctv_channels:
            cctv_channels_list.append({
                "channel": channel,
                "url": url,
                "logo": logo_url,  # ä½¿ç”¨æ–°çš„ç»Ÿä¸€Logo
                "group_title": "ğŸ“ºå¤®è§†é¢‘é“"
            })
        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯å«è§†é¢‘é“
        elif "å«è§†" in channel:  # å«è§†é¢‘é“
            satellite_channels.append({
                "channel": channel,
                "url": url,
                "logo": logo_url,  # ä½¿ç”¨æ–°çš„ç»Ÿä¸€Logo
                "group_title": "ğŸ“¡å«è§†é¢‘é“"
            })
        # 3. å¤„ç†åœ°æ–¹å°é¢‘é“
        else:
            # ä¼˜åŒ–ä¸­æ–‡å››è¿å­—åŒ¹é…
            province_scores = defaultdict(int)
            
            # 1. ç²¾ç¡®åŒ¹é…ï¼šæ£€æŸ¥é¢‘é“åç§°æ˜¯å¦å®Œæ•´åŒ…å«åœ¨é¢‘é“å­—ç¬¦ä¸²ä¸­
            for province, channels in province_channels.items():
                for channel_name in channels:
                    if channel_name in channel:
                        found_province = province
                        break
                if found_province:
                    break
            
            # 2. å››è¿å­—åŒ¹é…ï¼ˆä½¿ç”¨æ›´é•¿çš„ç‰¹å¾è¯æé«˜å‡†ç¡®æ€§ï¼‰
            if not found_province and len(channel) >= 4:
                # ä¸ºé¢‘é“åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„å››è¿å­—ç»„åˆ
                for i in range(len(channel) - 3):
                    quadgram = channel[i:i+4]
                    # æŸ¥æ‰¾åŒ¹é…çš„çœä»½
                    if quadgram in quadgram_to_province:
                        for province in quadgram_to_province[quadgram]:
                            # å››è¿å­—åŒ¹é…åŠ æ›´å¤šæƒé‡
                            province_scores[province] += 2
            
            # æ‰¾åˆ°åˆ†æ•°æœ€é«˜çš„çœä»½
            if province_scores:
                max_score = max(province_scores.values())
                best_provinces = [p for p, s in province_scores.items() if s == max_score]
                # å¦‚æœæœ‰å¤šä¸ªåˆ†æ•°ç›¸åŒçš„çœä»½ï¼Œé€‰æ‹©åç§°æœ€çŸ­çš„ï¼ˆæ›´å…·ä½“ï¼‰
                found_province = min(best_provinces, key=len) if best_provinces else None
            
            # æ ¹æ®åŒ¹é…ç»“æœåˆ†ç±»é¢‘é“
            if found_province:
                province_channels_list[found_province].append({
                    "channel": channel,
                    "url": url,
                    "logo": logo_url,  # ä½¿ç”¨æ–°çš„ç»Ÿä¸€Logo
                    "group_title": f"{found_province}"
                })
            else:
                # æœ€åçš„é˜²çº¿ï¼šæŸ¥æ‰¾åŒ…å«"å°"å­—çš„é¢‘é“
                if "å°" in channel:
                    province_channels_list["ğŸ§¯æ¨‚ç©å…¬ç¤¾"].append({
                        "channel": channel,
                        "url": url,
                        "logo": logo_url,  # ä½¿ç”¨æ–°çš„ç»Ÿä¸€Logo
                        "group_title": "ğŸ§¯æ¨‚ç©å…¬ç¤¾"
                    })
                else:
                    other_channels.append({
                        "channel": channel,
                        "url": url,
                        "logo": logo_url,  # ä½¿ç”¨æ–°çš„ç»Ÿä¸€Logo
                        "group_title": "ğŸ§¯æ¨‚ç©å…¬ç¤¾"
                    })

    # --- URLå»é‡é€»è¾‘å¼€å§‹ ---
    # æŒ‰åˆ†ç»„ä¼˜å…ˆçº§æ’åº (CCTV -> å«è§† -> çœä»½ -> å…¶ä»–)
    all_groups = [
        ("ğŸ“ºå¤®è§†é¢‘é“", cctv_channels_list),
        ("ğŸ“¡å«è§†é¢‘é“", satellite_channels)
    ]
    
    # æ·»åŠ çœä»½é¢‘é“ï¼ˆæŒ‰çœä»½åç§°æ’åºï¼‰
    for province in sorted(province_channels_list.keys()):
        if province == "ğŸ§¯æ¨‚ç©å…¬ç¤¾":
            continue  # å…¶ä»–é¢‘é“å•ç‹¬å¤„ç†
        all_groups.append((province, province_channels_list[province]))
    
    # æ·»åŠ å…¶ä»–é¢‘é“
    all_groups.append(("ğŸ§¯æ¨‚ç©å…¬ç¤¾", province_channels_list.get("ğŸ§¯æ¨‚ç©å…¬ç¤¾", [])))
    all_groups.append(("ğŸ§¯æ¨‚ç©å…¬ç¤¾", other_channels))

    # ä½¿ç”¨å­—å…¸æ ¹æ®URLå»é‡ï¼ˆä¿ç•™æ¯ä¸ªURLç¬¬ä¸€æ¬¡å‡ºç°çš„é¢‘é“ï¼‰
    seen_urls = set()
    deduped_channels = []
    
    for group_title, channels in all_groups:
        if not channels: continue
            
        # æ’åºå½“å‰åˆ†ç»„å†…çš„é¢‘é“
        channels.sort(key=lambda x: x["channel"])
        
        for channel_info in channels:
            url = channel_info["url"]
            if url not in seen_urls:
                seen_urls.add(url)
                deduped_channels.append({
                    "channel": channel_info["channel"],
                    "url": url,
                    "logo": channel_info["logo"],
                    "group_title": group_title
                })
    # --- URLå»é‡é€»è¾‘ç»“æŸ ---

    # å†™å…¥ M3U æ–‡ä»¶
    with open(m3u_filename, 'w', encoding='utf-8') as f:
        # æ·»åŠ å¸¦æœ‰æ‰€éœ€å±æ€§çš„æ ‡é¢˜è¡Œ
        f.write("#EXTM3U x-tvg-url=\"https://erw.shrimp.cloudns.biz/epg.xml\" catchup=\"append\" catchup-source=\"?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}\"\n")
        
        # å†™å…¥é¢‘é“ä¿¡æ¯
        for channel_info in deduped_channels:
            # ç”Ÿæˆé¢‘é“IDï¼ˆå»é™¤-ç¬¦å·çš„é¢‘é“åï¼‰
            channel_id = channel_info['channel'].replace('-', '')
            
            # å†™å…¥EXTINFè¡Œï¼Œä½¿ç”¨ç»Ÿä¸€çš„logoåœ°å€
            f.write(
                f"#EXTINF:-1 tvg-name=\"{channel_id}\" tvg-logo=\"{channel_info['logo']}\" group-title=\"{channel_info['group_title']}\",{channel_info['channel']}\n")
            
            # å†™å…¥é¢‘é“URL
            f.write(f"{channel_info['url']}\n")
            
    print(f"ğŸ‰ Generated M3U file: {m3u_filename}")
    
    # å†™å…¥ç»“æ„åŒ–çš„ TXT æ–‡ä»¶ (æŒ‰åˆ†ç»„ç»“æ„è¾“å‡º)
    with open(txt_filename, 'w', encoding='utf-8') as f:
        # 1. æŒ‰åˆ†ç»„æ”¶é›†é¢‘é“
        grouped_channels = defaultdict(list)
        for channel_info in deduped_channels:
            grouped_channels[channel_info['group_title']].append(channel_info)
        
        # 2. å®šä¹‰åˆ†ç»„æ’åºä¼˜å…ˆçº§
        group_order = [
            "ğŸ“›4KÂ·8Ké¢‘é“",
            "ğŸ“ºå¤®è§†é¢‘é“",
            "ğŸ“¡å«è§†é¢‘é“",
            "ğŸ’°ä»˜è´¹é¢‘é“",
            "ğŸæ•°å­—é¢‘é“",
            "ğŸ±NewTVé¢‘é“",
            "ğŸ³iHOTé¢‘é“",
            "ğŸ¦œDOXé¢‘é“",
            "ğŸŒCIBNé¢‘é“",
            "ğŸ’¾IPTVé¢‘é“",
            "ğŸšƒé‡åº†é¢‘é“",
            "ğŸš„å››å·é¢‘é“",
            "ğŸš…äº‘å—é¢‘é“",
            "ğŸšˆå®‰å¾½é¢‘é“",
            "ğŸšç¦å»ºé¢‘é“",
            "ğŸš‹ç”˜è‚ƒé¢‘é“",
            "ğŸšŒå¹¿ä¸œé¢‘é“",
            "ğŸšå¹¿è¥¿é¢‘é“",
            "ğŸšè´µå·é¢‘é“",
            "ğŸš‘æµ·å—é¢‘é“",
            "ğŸš’æ²³åŒ—é¢‘é“",
            "ğŸš“æ²³å—é¢‘é“",
            "ğŸš•é»‘é¾™æ±Ÿé¢‘é“",
            "ğŸš—æ¹–åŒ—é¢‘é“",
            "ğŸš™æ¹–å—é¢‘é“",
            "ğŸššå‰æ—é¢‘é“",
            "ğŸš‚æ±Ÿè‹é¢‘é“",
            "ğŸš›æ±Ÿè¥¿é¢‘é“",
            "ğŸšœè¾½å®é¢‘é“",
            "ğŸï¸å†…è’™å¤é¢‘é“",
            "ğŸï¸å®å¤é¢‘é“",
            "ğŸ›µé’æµ·é¢‘é“",
            "ğŸ¦½å±±ä¸œé¢‘é“",
            "ğŸ¦¼å±±è¥¿é¢‘é“",
            "ğŸ›ºé™•è¥¿é¢‘é“",
            "ğŸš²ä¸Šæµ·é¢‘é“",
            "ğŸ›´å¤©æ´¥é¢‘é“",
            "ğŸ›¹æ–°ç–†é¢‘é“",
            "ğŸšæµ™æ±Ÿé¢‘é“",
            "ğŸ›©ï¸åŒ—äº¬é¢‘é“",
            "ğŸï¸æ¸¯æ¾³å°é¢‘é“",
            "ğŸš¸å°‘å„¿é¢‘é“",
            "ğŸ¥å’ªå’•è§†é¢‘",
            "ğŸ¬å½±è§†å‰§é¢‘é“",
            "ğŸ®æ¸¸æˆé¢‘é“",
            "ğŸµéŸ³ä¹é¢‘é“",
            "ğŸ€ä½“è‚²é¢‘é“",
            "ğŸ›ç»å…¸å‰§åœº",
            "ğŸ¼ç†ŠçŒ«é¢‘é“",
            "ğŸšç›´æ’­ä¸­å›½",
            "ğŸ®å†å¹´æ˜¥æ™š",
            "ğŸªåŠ¨æ¼«é¢‘é“",
            "ğŸ§¯æ¨‚ç©å…¬ç¤¾"
        ]
        
        # 3. æŒ‰ä¼˜å…ˆçº§è¾“å‡ºåˆ†ç»„
        for group in group_order:
            if group in grouped_channels and grouped_channels[group]:
                # ä¿®æ”¹ä¸º: è¾“å‡ºåˆ†ç»„æ ‡é¢˜è¡Œæ ¼å¼ä¸º "åˆ†ç»„æ ‡é¢˜,#genre#"
                f.write(f"{group},#genre#\n")
                
                # æŒ‰é¢‘é“åç§°æ’åºå¹¶è¾“å‡º
                channels = sorted(grouped_channels[group], key=lambda x: x['channel'])
                for channel_info in channels:
                    f.write(f"{channel_info['channel']},{channel_info['url']}\n")
        
        # 4. å¤„ç†å¯èƒ½æ¼æ‰çš„åˆ†ç»„
        for group, channels in grouped_channels.items():
            if group not in group_order and channels:
                # ä¿®æ”¹ä¸º: è¾“å‡ºåˆ†ç»„æ ‡é¢˜è¡Œæ ¼å¼ä¸º "åˆ†ç»„æ ‡é¢˜,#genre#"
                f.write(f"{group},#genre#\n")
                
                # æŒ‰é¢‘é“åç§°æ’åºå¹¶è¾“å‡º
                channels = sorted(channels, key=lambda x: x['channel'])
                for channel_info in channels:
                    f.write(f"{channel_info['channel']},{channel_info['url']}\n")
                    
    print(f"ğŸ‰ Generated structured TXT file: {txt_filename}")


# ä¸»å‡½æ•°ï¼šå¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
async def main(file_urls, cctv_channel_file, province_channel_files):
    """ä¸»å‡½æ•°å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    # åŠ è½½ CCTV é¢‘é“åˆ—è¡¨
    cctv_channels = load_cctv_channels(cctv_channel_file)

    # åŠ è½½å¤šä¸ªçœä»½é¢‘é“åˆ—è¡¨
    province_channels = load_province_channels(province_channel_files)

    all_valid_urls = []

    semaphore = asyncio.Semaphore(CONFIG["max_parallel"])

    async def limited_task(task):
        async with semaphore:
            return await task

    tasks = []
    for file_url in file_urls:
        if file_url.endswith(('.m3u', '.m3u8')):
            tasks.append(limited_task(read_and_test_file(file_url, is_m3u=True)))
        elif file_url.endswith('.txt'):
            tasks.append(limited_task(read_and_test_file(file_url, is_m3u=False)))
        else:
            continue

    results = await asyncio.gather(*tasks)
    for valid_urls in results:
        all_valid_urls.extend(valid_urls)

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    generate_output_files(
        all_valid_urls, 
        cctv_channels, 
        province_channels, 
        CONFIG["output_m3u"],
        CONFIG["output_txt"]
    )


if __name__ == "__main__":
    # IPTV æ–‡ä»¶ URLï¼ˆæ‚¨å¯ä»¥æ·»åŠ è‡ªå·±çš„æ–‡ä»¶ URL åˆ—è¡¨ï¼‰
    file_urls = [
        "https://raw.githubusercontent.com/ssili126/tv/main/itvlist.txt"
    ]

    # CCTV é¢‘é“æ–‡ä»¶ï¼ˆä¾‹å¦‚ IPTV/CCTV.txtï¼‰
    cctv_channel_file = ".github/workflows/iTV/CCTV.txt"

    # çœä»½é¢‘é“æ–‡ä»¶åˆ—è¡¨
    province_channel_files = [
        ".github/workflows/iTV/ğŸ“›4KÂ·8Ké¢‘é“.txt",
        ".github/workflows/iTV/ğŸ’°ä»˜è´¹é¢‘é“.txt",
        ".github/workflows/iTV/ğŸæ•°å­—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ±NewTVé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ³iHOTé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ¦œDOXé¢‘é“.txt",
        ".github/workflows/iTV/ğŸŒCIBNé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ’¾IPTVé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ“¡å«è§†é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšƒé‡åº†é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš„å››å·é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš…äº‘å—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšˆå®‰å¾½é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšç¦å»ºé¢‘é“.txt",
        ".github/workflows/iTV/ğŸš‹ç”˜è‚ƒé¢‘é“.txt",
        ".github/workflows/iTV/ğŸšŒå¹¿ä¸œé¢‘é“.txt",
        ".github/workflows/iTV/ğŸšå¹¿è¥¿é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšè´µå·é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš‘æµ·å—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš’æ²³åŒ—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš“æ²³å—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš•é»‘é¾™æ±Ÿé¢‘é“.txt",
        ".github/workflows/iTV/ğŸš—æ¹–åŒ—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš™æ¹–å—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸššå‰æ—é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš‚æ±Ÿè‹é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš›æ±Ÿè¥¿é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšœè¾½å®é¢‘é“.txt",
        ".github/workflows/iTV/ğŸï¸å†…è’™å¤é¢‘é“.txt",
        ".github/workflows/iTV/ğŸï¸å®å¤é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›µé’æµ·é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ¦½å±±ä¸œé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ¦¼å±±è¥¿é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›ºé™•è¥¿é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš²ä¸Šæµ·é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›´å¤©æ´¥é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›¹æ–°ç–†é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšæµ™æ±Ÿé¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›©ï¸åŒ—äº¬é¢‘é“.txt",
        ".github/workflows/iTV/ğŸï¸æ¸¯æ¾³å°é¢‘é“.txt",
        ".github/workflows/iTV/ğŸš¸å°‘å„¿é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ¥å’ªå’•è§†é¢‘.txt",
        ".github/workflows/iTV/ğŸ¬å½±è§†å‰§é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ®æ¸¸æˆé¢‘é“.txt",
        ".github/workflows/iTV/ğŸµéŸ³ä¹é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ€ä½“è‚²é¢‘é“.txt",
        ".github/workflows/iTV/ğŸ›ç»å…¸å‰§åœº.txt",
        ".github/workflows/iTV/ğŸ¼ç†ŠçŒ«é¢‘é“.txt",
        ".github/workflows/iTV/ğŸšç›´æ’­ä¸­å›½.txt",
        ".github/workflows/iTV/ğŸ®å†å¹´æ˜¥æ™š.txt",
        ".github/workflows/iTV/ğŸªåŠ¨æ¼«é¢‘é“.txt"
    ]

    # æ‰§è¡Œä¸»å‡½æ•°
    asyncio.run(main(file_urls, cctv_channel_file, province_channel_files))